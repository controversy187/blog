<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Brett Fitzgerald</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Brett Fitzgerald</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
    
    
    <atom:link href="http://localhost:1313/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Obsidian, MCP Servers, and Retreiving Data from my Second Brain</title>
      <link>http://localhost:1313/posts/mcp-server-experiences/</link>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0500</pubDate>
      <guid>http://localhost:1313/posts/mcp-server-experiences/</guid>
      <description>&lt;h2 id=&#34;my-journey-with-mcp-servers&#34;&gt;My Journey with MCP Servers&lt;/h2&gt;
&lt;p&gt;I recently learned of MCP servers through a &lt;a href=&#34;https://news.ycombinator.com/item?id=43410866&#34;&gt;post on Hacker News&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;
&lt;p&gt;[Explain what MCP servers are and why you started using them]&lt;/p&gt;
&lt;h3 id=&#34;setup-and-configuration&#34;&gt;Setup and Configuration&lt;/h3&gt;
&lt;p&gt;[Discuss your setup process and initial configuration]&lt;/p&gt;
&lt;h3 id=&#34;challenges-encountered&#34;&gt;Challenges Encountered&lt;/h3&gt;
&lt;p&gt;[Share any problems or challenges you faced]&lt;/p&gt;
&lt;h3 id=&#34;performance-analysis&#34;&gt;Performance Analysis&lt;/h3&gt;
&lt;p&gt;[Talk about the performance aspects of MCP servers in your environment]&lt;/p&gt;
&lt;h3 id=&#34;lessons-learned&#34;&gt;Lessons Learned&lt;/h3&gt;
&lt;p&gt;[Share insights and lessons from your experience]&lt;/p&gt;
&lt;h3 id=&#34;recommendations&#34;&gt;Recommendations&lt;/h3&gt;
&lt;p&gt;[Provide recommendations for others considering MCP servers]&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;[Summarize your overall experience and final thoughts]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Speed Controls in Davinci Resolve</title>
      <link>http://localhost:1313/posts/speed-controls-in-davinci-resolve/</link>
      <pubDate>Tue, 25 Feb 2025 10:22:19 -0500</pubDate>
      <guid>http://localhost:1313/posts/speed-controls-in-davinci-resolve/</guid>
      <description>&lt;h2 id=&#34;controlling-clip-speed-in-davinci-resolve&#34;&gt;Controlling clip speed in DaVinci Resolve&lt;/h2&gt;
&lt;p&gt;This is the process I just learned for adjusting speed within clips in DaVinci Resolve 19. This isn&amp;rsquo;t an adjustment for an entire clip, but portions of a clip. You can see the use-case in several of the shots &lt;a href=&#34;https://www.youtube.com/watch?v=IymFBGIcqcI&#34;&gt;here&lt;/a&gt;. In the past, I’d cut a clip where I wanted the speed to change and adjust the middle section’s speed, turning one clip into three.&lt;/p&gt;
&lt;h2 id=&#34;adjust-clip-speed-using-time-controls&#34;&gt;Adjust clip speed using Time Controls&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ll use the last clip from the above video as a tutorial. The first step is to select the clip you want to adjust on your timeline, in the Edit tab. Right-click the clip and select &amp;ldquo;Retime Controls&amp;rdquo; (ctrl+R on Windows).
&lt;img alt=&#34;Context Menu&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/speed-controls-in-davinci-resolve/context_menu.jpg&#34;&gt;
You’ll see blue triangles pointing right. Move your playhead (the timeline cursor showing your current frame) to the frame where you want the speed change to start. At the bottom of the clip on the timeline, you should see an indicator that shows &amp;ldquo;100%&amp;rdquo; followed by a downward facing triangle. Clicking that triangle pops open a context menu. From there, select &amp;ldquo;Add Speed Point.&amp;rdquo;
&lt;img alt=&#34;Add Speed Point&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/speed-controls-in-davinci-resolve/add-speed-point.jpg&#34;&gt;
This functions like a keyframe, if you&amp;rsquo;ve done other video effects or editing in the past. With that divider there, you can see the clip is effectively bisected at the frame you have selected, and each side shows 100%. Now, you can select either side (I&amp;rsquo;ll use the right-side) to adjust the speed. Clicking the black down arrow there allows you &amp;ldquo;Change Speed&amp;rdquo; to a handful of preset options. I&amp;rsquo;ll select 25%.
&lt;img alt=&#34;25 Percent&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/speed-controls-in-davinci-resolve/25-percent.jpg&#34;&gt;
Now the arrows change color and extend. Effectively, you have created another clip, with a slower speed. To resume normal speed later, move your playhead to that spot and add another speed point (via the black arrow). Then reset the right side to 100%. Your final result is a clip that runs at normal speed, slows down in the middle, and resumes normal speed at the end!
&lt;img alt=&#34;Final Cut&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/speed-controls-in-davinci-resolve/final-edit.jpg&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Video Compression Analysis</title>
      <link>http://localhost:1313/posts/video-compression-analysis/</link>
      <pubDate>Wed, 12 Feb 2025 05:00:00 -0500</pubDate>
      <guid>http://localhost:1313/posts/video-compression-analysis/</guid>
      <description>&lt;h2 id=&#34;a-videography-hobbyist&#34;&gt;A videography hobbyist&lt;/h2&gt;
&lt;p&gt;In my free time, I like shooting videos of my family&amp;rsquo;s adventures and doing some basic editing. I shoot on my cellphone and a GoPro. For the past several years, I have rendered my final projects in 1080p at 24 frames per second. I liked the ability to shoot in 4k and still &amp;ldquo;zoom in&amp;rdquo; digitally to 1080. That also let me shoot slow motion video at 1080, and match my final render resolution. I recently got a newer GoPro, so now I can shoot 4k at 120 fps, which allows me slow down to 20% speed if I render my final project at 24 fps.&lt;/p&gt;
&lt;p&gt;With this advent of being able to shoot slow motion in 4k resolution, I decided to start rendering my projects in 4k, by default. I&amp;rsquo;m also experimenting with doing very quick edits, just splicing the day&amp;rsquo;s footage together, applying automatic color balancing per-clip, and then rendering at 60fps. This is more of a &amp;ldquo;home movie&amp;rdquo; of a day or event, rather than a curated, edited highlight video. I do all this in Davinci Resolve. Previously, I would be able to render my 1080 videos at 24fps and be happy with the file size and quality of picture. Now that I am rendering at four times the resolution and two and a half times the framerate, my output filesize has ballooned, and I need to pay better attention to my compression.&lt;/p&gt;
&lt;p&gt;I want to find a good balance between output filesize and quality for my home movies.&lt;/p&gt;
&lt;h2 id=&#34;comparison-of-projects&#34;&gt;Comparison of projects&lt;/h2&gt;
&lt;p&gt;In the past, I would shoot for around 100 megabytes per minute of rendered video. So a 5 minute video, rendered at a resolution of 1080, at 24 frames per second would come in around 500 MB. For videos I really spent time on, I would bump up my quality settings and I&amp;rsquo;d be happy with a 3 gig file for a 5 minute video.&lt;/p&gt;
&lt;p&gt;I recently rendered a video using Davinci Resolve&amp;rsquo;s 4k &amp;ldquo;Master&amp;rdquo; preset. So a resolution of 4k, at 60 frames per second, and a duration of 22 minutes came in at a whopping 75 gigabytes (~3.4 GB / min). I used Resolve&amp;rsquo;s &amp;ldquo;YouTube&amp;rdquo; preset, and that reduced the filesize to 1.8 GB.(~80 MB / min). That is a significant difference!&lt;/p&gt;
&lt;p&gt;For reference, my input files, shot on the GoPro Hero 13 were shot in 4k, mostly at 120 fps. They totaled 18.2GB, so my rendered file was actually four times larger than my source material!&lt;/p&gt;
&lt;p&gt;The two questions I have are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What is the difference in output files between these two?&lt;/li&gt;
&lt;li&gt;Is there a noticeable difference in visible quality?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;differences-in-objective-data&#34;&gt;Differences in objective data&lt;/h2&gt;
&lt;p&gt;I wrote a &lt;a href=&#34;https://gist.github.com/controversy187/0d33948ba3afeb5ba4c4d2fb9ae8113f&#34;&gt;python script&lt;/a&gt; that compares various aspects of the videos. I also ran them through the various presets in &lt;a href=&#34;https://handbrake.fr/&#34;&gt;Handbrake&lt;/a&gt; to see how they compare. The video is 21:49 long. These are the results of that comparison, in order of increasing filesize.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Filename&lt;/th&gt;
&lt;th&gt;Bitrate (kbps)&lt;/th&gt;
&lt;th&gt;Resolution&lt;/th&gt;
&lt;th&gt;Framerate (FPS)&lt;/th&gt;
&lt;th&gt;Video Codec&lt;/th&gt;
&lt;th&gt;Filesize&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Source Video (Sample).MP4&lt;/td&gt;
&lt;td&gt;120000&lt;/td&gt;
&lt;td&gt;3840x2160&lt;/td&gt;
&lt;td&gt;119.88&lt;/td&gt;
&lt;td&gt;hevc&lt;/td&gt;
&lt;td&gt;67 &lt;strong&gt;MB&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resolve - YouTube - h264.mp4&lt;/td&gt;
&lt;td&gt;11618&lt;/td&gt;
&lt;td&gt;3840x2160&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;h264&lt;/td&gt;
&lt;td&gt;1.8 GB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resolve - YouTube - h265.mp4&lt;/td&gt;
&lt;td&gt;10566&lt;/td&gt;
&lt;td&gt;3840x2160&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;hevc&lt;/td&gt;
&lt;td&gt;1.7 GB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resolve - Master - h264 - HandBrake - Fast.mp4&lt;/td&gt;
&lt;td&gt;37948&lt;/td&gt;
&lt;td&gt;3840x2160&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;hevc&lt;/td&gt;
&lt;td&gt;6 GB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resolve - Master - h264 - HandBrake - VeryFast.mp4&lt;/td&gt;
&lt;td&gt;41025&lt;/td&gt;
&lt;td&gt;3840x2160&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;hevc&lt;/td&gt;
&lt;td&gt;6.5 GB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resolve - Master - h264 - HandBrake - HQ.mp4&lt;/td&gt;
&lt;td&gt;57001&lt;/td&gt;
&lt;td&gt;3840x2160&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;hevc&lt;/td&gt;
&lt;td&gt;9 GB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resolve - Master - h264 - HandBrake - Super HQ.mp4&lt;/td&gt;
&lt;td&gt;78210&lt;/td&gt;
&lt;td&gt;3840x2160&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;hevc&lt;/td&gt;
&lt;td&gt;12.5 GB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resolve - Master - h265.mp4&lt;/td&gt;
&lt;td&gt;473927&lt;/td&gt;
&lt;td&gt;3840x2160&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;h264&lt;/td&gt;
&lt;td&gt;75.7 GB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resolve - Master - h264.mp4&lt;/td&gt;
&lt;td&gt;474208&lt;/td&gt;
&lt;td&gt;3840x2160&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;h264&lt;/td&gt;
&lt;td&gt;75.8 GB&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Obviously, Handbrake is going a good job at compressing the video, and lowering the bitrate, thus reducing filesize. But how do the videos actually &lt;em&gt;look&lt;/em&gt;?&lt;/p&gt;
&lt;h2 id=&#34;subjective-comparison&#34;&gt;Subjective comparison&lt;/h2&gt;
&lt;p&gt;Here are images captured from each of the above videos.&lt;/p&gt;
&lt;div id=&#34;gallery&#34;&gt;
  &lt;a href=&#34;images/Resolve - YouTube - h264.jpg&#34; data-sub-html=&#34;Davinci Resolve, YouTube preset, h264&#34;&gt;
  	&lt;img src=&#34;images/Resolve - YouTube - h264.jpg&#34; width=&#34;100%&#34;&gt;
  	Davinci Resolve, YouTube preset, h264
  &lt;/a&gt;
  &lt;a href=&#34;images/Resolve - YouTube - h265.jpg&#34; data-sub-html=&#34;Davinci Resolve, YouTube preset, h265&#34;&gt;
  	&lt;img src=&#34;images/Resolve - YouTube - h265.jpg&#34; width=&#34;100%&#34;&gt;
  	Davinci Resolve, YouTube Preset, h265
  &lt;/a&gt;
  &lt;a href=&#34;images/Resolve - Master - h264 - HandBrake - Fast.jpg&#34; data-sub-html=&#34;Handbrake, Fast&#34;&gt;
  	&lt;img src=&#34;images/Resolve - Master - h264 - HandBrake - Fast.jpg&#34; width=&#34;100%&#34;&gt;
  	Handbrake, Fast
  &lt;/a&gt;
  &lt;a href=&#34;images/Resolve - Master - h264 - HandBrake - VeryFast.jpg&#34; data-sub-html=&#34;Handbrake, Very Fast&#34;&gt;
  	&lt;img src=&#34;images/Resolve - Master - h264 - HandBrake - VeryFast.jpg&#34; width=&#34;100%&#34;&gt;
  	Handbrake, Very Fast
  &lt;/a&gt;
  &lt;a href=&#34;images/Resolve - Master - h264 - HandBrake - HQ.jpg&#34; data-sub-html=&#34;Handbrake, High Quality&#34;&gt;
  	&lt;img src=&#34;images/Resolve - Master - h264 - HandBrake - HQ.jpg&#34; width=&#34;100%&#34;&gt;
  	Handbrake, High Quality
  &lt;/a&gt;
  &lt;a href=&#34;images/Resolve - Master - h264 - HandBrake - Super HQ.jpg&#34; data-sub-html=&#34;Handbrake, Super High Quality&#34;&gt;
  	&lt;img src=&#34;images/Resolve - Master - h264 - HandBrake - Super HQ.jpg&#34; width=&#34;100%&#34;&gt;
  	Handbrake, Super High Quality
  &lt;/a&gt;
  &lt;a href=&#34;images/Resolve - Master - h264.jpg&#34; data-sub-html=&#34;Davinci Resolve, Master - h264&#34;&gt;
  	&lt;img src=&#34;images/Resolve - Master - h264.jpg&#34; width=&#34;100%&#34;&gt;
  	Davinci Resolve, Master - h264
  &lt;/a&gt;
  &lt;a href=&#34;images/Resolve - Master - h265.jpg&#34; data-sub-html=&#34;Davinci Resolve, Master - h265&#34;&gt;
  	&lt;img src=&#34;images/Resolve - Master - h265.jpg&#34; width=&#34;100%&#34;&gt;
  	Davinci Resolve, Master - h265
  &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;In these very specific examples, you can immediately see that the DaVinci Resolve YouTube presets are not good. The h264 version shows artifiacts on the right side of the frame and all detail in the snow on the ground is completely lost. Interestingly, the h265 codec doesn&amp;rsquo;t lose as much detail, and is slightly smaller. In the case were you need a small file, it seems like the h265 does a better job at these lower bitrates.&lt;/p&gt;
&lt;p&gt;When we jump up into the Handbrake re-encodes, things get noticeably better. Honestly, from the still frames it&amp;rsquo;s very hard (for me) to tell the difference between these images, all the way through to the masters. Even when I watch the playback of the videos themselves, I&amp;rsquo;m hard pressed to see any difference. It could be that the source clips themselves only have a 120Mbps bitrate, and we&amp;rsquo;re re-encoding at a higher bitrate for the masters (474Mbps).&lt;/p&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;To really judge this fairly, I probably should have rendered everything out from DaVinci Resolve and manually adjusted the bitrate. Based on these tests, though, I&amp;rsquo;m not seeing a noticeable loss in quality between the 474Mbps best quality from Resolve and a re-encoded to 78Mbps in Handbrake. For the time being, I&amp;rsquo;m planning to render out from Resolve and limiting my bitrate to 80Mbps. That&amp;rsquo;s only 590 MB or so per minute of video, which isn&amp;rsquo;t bad for what I&amp;rsquo;m doing with them.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automatic Music Markers in Davinci Resolve</title>
      <link>http://localhost:1313/posts/automatic-music-markers-in-davinci-resolve/</link>
      <pubDate>Mon, 10 Feb 2025 15:20:00 -0500</pubDate>
      <guid>http://localhost:1313/posts/automatic-music-markers-in-davinci-resolve/</guid>
      <description>&lt;h2 id=&#34;making-music-videos&#34;&gt;Making Music Videos&lt;/h2&gt;
&lt;p&gt;I am very formulaic in how I make videos for my personal library.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Shoot some footage on my cell phone and/or a GoPro.&lt;/li&gt;
&lt;li&gt;Find a song that captures the feeling of the event.&lt;/li&gt;
&lt;li&gt;Import all the clips from the event to DaVinci Resolve, and the song selected above.&lt;/li&gt;
&lt;li&gt;Add the song to the timeline.&lt;/li&gt;
&lt;li&gt;I then go through the clips, usually chronologically, and see which ones are &amp;ldquo;cool enough&amp;rdquo; to make it the cut.&lt;/li&gt;
&lt;li&gt;If I end up with more clips than I have time for, ruthlessly cut clips until I have the right amount for the song.&lt;/li&gt;
&lt;li&gt;Manually drag clip start and end points to align with the measures of the song.&lt;/li&gt;
&lt;li&gt;Apply Autolevels to each clip, based on the midpoint of the clip (I&amp;rsquo;m red/green color blind and haven&amp;rsquo;t invest any time into figuring out how to color grade).&lt;/li&gt;
&lt;li&gt;Export and Upload.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It&amp;rsquo;s time consuming, but I usually enjoy it. One part that is always frustrating, though, is aligning the cuts in the clips to the measures in the song. I play the timeline and tap the beat on my desk, usually counting out the beats. If I&amp;rsquo;m lucky, there is a visible spike in the waveform on the beats, so I can drag my video clip boundaries to align with them. Sometimes I just have to guess where they are. The whole point is that a generic video like this will draw someone in mre effectively if the video is synchronized with the audio. It&amp;rsquo;s hard to describe, but without the synchronization, the video just sort of plays and the music creates a general feeling. When the timings line up, it feels like the music is pushing the visuals forward, creating more of an emotional resonance with the viewer. That&amp;rsquo;s my experience, at least.&lt;/p&gt;
&lt;p&gt;I wanted to figure out how I could make it easier to find those beats in the audio, and the measure boundaries, so I could speed my workflow.&lt;/p&gt;
&lt;h2 id=&#34;making-a-click-track&#34;&gt;Making a click-track&lt;/h2&gt;
&lt;p&gt;My first attempt was to use python to analyize the audio file of the song I was using. I figured that there is likely some library out there that would determine the beats and hopefully the measures. I could then use that information to generate a new audio file. A single &amp;ldquo;click&amp;rdquo; sound on the beats, and a higher pitched click on the first beat of each measure. With that audio file, I could add it to the timeline in Resolve, mute it, and have a very clear visual to align my video clips to.&lt;/p&gt;
&lt;p&gt;With some help from Claude.ai and ChatGPT, I played with the librosa libary. It was suprisingly accurate at finding the beats and generating a click track. The biggest hurdle I ran into happened during musical transitions. It seems that librosa analyizes the music for acoustic &amp;ldquo;density&amp;rdquo;. Music is generally more &amp;ldquo;dense&amp;rdquo; on the beats, which is where louder drums are hit, guitar chords are strummed, keys are struck, and generally most or all of the instruments &amp;ldquo;happen&amp;rdquo; at the same time.&lt;/p&gt;
&lt;p&gt;For example, a guitar chord would be played on a beat, and then it is left to resonate until the next beat, or possibly a divison of the next beat. So with most of the instruments striking on the beats, those moments are more dense. Any given song will usually have a relatively consistent feel, and it seems like librosa does a good job of determining that feel, finding those dense moments and aligning them to the beats. Where it struggled for me is when the general density of the music changed.&lt;/p&gt;
&lt;p&gt;I was testing with The Allman Brother&amp;rsquo;s song Ramblin&amp;rsquo; Man. This song starts with a clearly defined introduction which is mainly electric guitar for a couple of measures, and then the full band comes in. The problem is that librosa seems to normalize to the relatively low density of the electric guitar-only intro, and correctly identifies the beats. But once the full band comes in, the density goes up significantly, and librosa identifies extra beats for a short period, until it renormalizes to the higher density of the song. That throws off the counts of our beats, and then our measures are no longer in alignment.&lt;/p&gt;
&lt;p&gt;I played with several parameters of librosa, but never got to satisfactory results.&lt;/p&gt;
&lt;h2 id=&#34;making-project-markers&#34;&gt;Making project markers&lt;/h2&gt;
&lt;p&gt;DaVinci Resolve (and most other populary video editing tools) support a feature called Markers. I haven&amp;rsquo;t used them before, so this is my first foray into them. The idea is that you can set specific markers on your timeline and then snap clips to them.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Build a Large Language Model From Scratch</title>
      <link>http://localhost:1313/posts/build-a-large-language-model/</link>
      <pubDate>Thu, 06 Feb 2025 12:57:27 -0500</pubDate>
      <guid>http://localhost:1313/posts/build-a-large-language-model/</guid>
      <description>&lt;h2 id=&#34;building-a-large-language-model-from-scratch&#34;&gt;Building a large language model from scratch&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m a machine learning / A.I. hobbyist. The technologies fascinate me, and I can&amp;rsquo;t seem to learn enough about them. Sebastian Raschka&amp;rsquo;s book, Build a Large Language Model (From Scratch) caught my eye. I don&amp;rsquo;t recall how I stumbled on it, but I found it when it was still in early access from Manning Publications. I purchased it, and started working through it as the final chapters were being written and released. I just completed the book and all the included work and loved every minute of it.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Build a Large Language Model From Scratch by Sebastian Raschka&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/build-a-large-language-model/llm-book.jpg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;my-approach&#34;&gt;My approach&lt;/h2&gt;
&lt;p&gt;A while ago, I read some advice about learning programming from digital books and tutorials. The advice was to never copy and paste code from samples but to hand-type all the code. I took that approach with this book. I typed every single line of code (except for a couple of blocks which were highly repetitive and long). You can see all my work here: &lt;a href=&#34;https://github.com/controversy187/build-a-large-language-model&#34;&gt;https://github.com/controversy187/build-a-large-language-model&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I did my best to work in section chunks. I didn&amp;rsquo;t want to start a section unless I had the time dedicated to completing it. Some sections are pretty short, others are fairly involved and time-consuming.&lt;/p&gt;
&lt;p&gt;I built this in Jupyter Notebooks on my laptop, which is pretty underpowered for this type of work. The premise of the book was that you can build an LLM on consumer hardware, and it can perform decently well. As I&amp;rsquo;m writing this, I&amp;rsquo;m currently fine-tuning my model locally. My model is about 50 steps into a 230-step tuning, and I just crossed the 20-minute execution time mark. The earlier code samples ran quicker, but the last few sections used larger models, which slowed things down considerably.&lt;/p&gt;
&lt;p&gt;I didn&amp;rsquo;t do most of the supplemental exercises. I tend to have an &amp;ldquo;I want to do ALL THE THINGS!&amp;rdquo; personality. The drawback is that if I take the time to do all the things, I eventually get long-term distracted and never actually finish what I started. So I sort of rushed through this book. I even took several weeks off around Christmas and New Year&amp;rsquo;s. But I got back into it and powered through the last few chapters.&lt;/p&gt;
&lt;p&gt;So, more or less, I read through the chapters and wrote all the mandatory coding assignments.&lt;/p&gt;
&lt;h2 id=&#34;learnings&#34;&gt;Learnings&lt;/h2&gt;
&lt;p&gt;What can I tell you about large language models? A lot more than I could before I started this book, but certainly not all the things the author attempted to teach me. I&amp;rsquo;ll summarize my understanding, but I could be wrong about some of these things, and I most certainly forgot or misunderstood others.&lt;/p&gt;
&lt;h3 id=&#34;tokenization--vocabulary&#34;&gt;Tokenization &amp;amp; Vocabulary&lt;/h3&gt;
&lt;p&gt;A large language model starts its life by building a vocabulary of text. A massive amount of text is distilled down into a list of unique words. Each word is then translated into an integer because computers like numbers more than they like words. This process is referred to as &amp;ldquo;tokenization&amp;rdquo;, where the word is replaced with a numerical token. So now we have a list of unique tokens, which is the vocabulary of the large language model.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Build a more advanced tokenizer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hello, world. Is this-- a test?&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;([,.:;?_!&amp;#34;()&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;]|--|\s)&amp;#39;&lt;/span&gt;, text)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [item&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip() &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; item &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; result &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; item&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip()]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(result)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Outputs &amp;#34;[&amp;#39;Hello&amp;#39;, &amp;#39;,&amp;#39;, &amp;#39;world&amp;#39;, &amp;#39;.&amp;#39;, &amp;#39;Is&amp;#39;, &amp;#39;this&amp;#39;, &amp;#39;--&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;test&amp;#39;, &amp;#39;?&amp;#39;]&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;all_words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sorted(set(result))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vocab_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(all_words)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(vocab_size)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Outputs 10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Display the first 51 tokens in our vocabulary.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vocab &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {token:integer &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; integer,token &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate(all_words)}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i, item &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate(vocab&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items()):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(item)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Outputs:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;,&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;--&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;?&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Hello&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Is&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;test&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;this&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;world&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# In this example, the id 9 represents the word &amp;#34;world&amp;#34;. 5 represents &amp;#34;Is&amp;#34;. etc.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is where my understanding gets fuzzy. We didn&amp;rsquo;t get very far before that happened, &amp;rsquo;eh? Now, we take that massive amount of text we were using earlier to create the vocabulary (or a subset, or totally different text), and we tokenize the entire text. We do this by using the vocabulary we built previously and substituting the words in the training text for their equivalent token value. This is now our training text.&lt;/p&gt;
&lt;h3 id=&#34;model-training--relationships&#34;&gt;Model Training &amp;amp; Relationships&lt;/h3&gt;
&lt;p&gt;With that complete, we can &amp;ldquo;train&amp;rdquo; the model. This process involves taking each token in the vocabulary and building a relationship to each other token in the vocabulary, based on those tokens&amp;rsquo; relative positions to each other in the training text. So if the word &amp;ldquo;cat&amp;rdquo; is followed by the word &amp;ldquo;jump&amp;rdquo;, the model records that relationship. But it also records the relationship of the word &amp;ldquo;cat&amp;rdquo; to other words in the text. So &amp;ldquo;jump&amp;rdquo; follows &amp;ldquo;cat&amp;rdquo;, but maybe it does so more frequently when they are close to the word &amp;ldquo;mouse&amp;rdquo;. And maybe less frequently when they are close to the word &amp;ldquo;nap&amp;rdquo;. Recording ALL these relationships would require a massive dataset, so the relationships are mathematically reduced and approximated. There are definitely more technical terms to use, and the book went into them. I definitely forget them, though.&lt;/p&gt;
&lt;h3 id=&#34;text-generation-process&#34;&gt;Text Generation Process&lt;/h3&gt;
&lt;p&gt;Now, if you provide a starter text to the model, it will try to complete the text for you. Continuing our example, if I gave the model the text &amp;ldquo;My cat saw a mouse and it&amp;rdquo;, based on the word cat being close to the word mouse, it might predict the word &amp;ldquo;jumped&amp;rdquo; to come next. So it appends the word &amp;ldquo;jumped&amp;rdquo; to the text I submitted, and then it takes that whole new sentence and feeds it back into itself. So now the input text is &amp;ldquo;My cat saw a mouse and it jumped&amp;rdquo;. The next output word could be &amp;ldquo;on&amp;rdquo;, so it appends that word and feeds this concatenated output back into its input.&lt;/p&gt;
&lt;p&gt;&lt;del&gt;Every time it does a loop like this, it tokenizes the entire input (or up to a limit, known as a context limit or context window) and then calculates the most likely next token, then converts it all back to text for us to read.&lt;/del&gt; &lt;em&gt;&lt;a href=&#34;#update-2025-02-17&#34;&gt;See update&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;model-weights--distribution&#34;&gt;Model Weights &amp;amp; Distribution&lt;/h3&gt;
&lt;p&gt;&lt;del&gt;Saving all those relationships between the tokens are known as the &amp;ldquo;weights&amp;rdquo; of the model.&lt;/del&gt; &lt;em&gt;&lt;a href=&#34;#update-2025-02-17&#34;&gt;See update&lt;/a&gt;&lt;/em&gt; Those can be distributed, so if you train a model on a given training text, you can give that to your friends and they can use those model weights to predict text similar to that training text.&lt;/p&gt;
&lt;h3 id=&#34;fine-tuning&#34;&gt;Fine Tuning&lt;/h3&gt;
&lt;p&gt;Fine-tuning is the process of training a model for specific&amp;hellip; things. My mind is getting fuzzier here, so I&amp;rsquo;m not going to go into this deeper. Suffice it to say, that you start with a base language model and continue to train it using specific input and output pairs. In the book, we built a spam classifier that determined if a given message was spam or not, as well as a model that will follow instructions. That&amp;rsquo;s actually the one that&amp;rsquo;s being trained right now as I write this post, so I&amp;rsquo;m not sure how it will turn out. Based on the fact that it&amp;rsquo;s published in a book, I think it will come out just fine.&lt;/p&gt;
&lt;p&gt;So while I&amp;rsquo;m not completely done with the book, I&amp;rsquo;m very nearly there. I did learn a lot of great concepts, although obviously some of them weren&amp;rsquo;t retained. It would probably behoove me to go back through the book again and quickly breeze through it, in order to refresh my memory and cement my learnings.&lt;/p&gt;
&lt;h2 id=&#34;meta-learnings&#34;&gt;Meta learnings&lt;/h2&gt;
&lt;p&gt;Other than the technical aspects of Large Language Models, what else did I learn through this experience?&lt;/p&gt;
&lt;p&gt;Through my experiment with typing all the code samples by hand, I can say that my time would have been better spent with a different approach. If I do this again, I&amp;rsquo;ll probably not type all the code snippets, but rather &amp;ldquo;type&amp;rdquo; them in my mind, and really understand what each line does. The times I learned the most were actually when I made a typo and had to go back through my code to debug it. That forced me to understand what was happening so I could figure out what went wrong.&lt;/p&gt;
&lt;p&gt;I learn better with paper, rather than a digital book. I don&amp;rsquo;t know why. I had both available to me, and I read the first couple of chapters in the paper book. That information stuck better. Maybe because it was earlier in the book and simpler to understand, or maybe the format played into it. But I enjoyed it better, regardless.&lt;/p&gt;
&lt;p&gt;I didn&amp;rsquo;t have to &amp;ldquo;figure out&amp;rdquo; anything, and I think that hampered my learning. There are supplemental exercises in the book, where the author gives you a problem and you have to figure out how to solve it. The answers are given in his GitHub repository. That would have slowed me down a lot, but I&amp;rsquo;m very confident that I would have learned the material better.&lt;/p&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;What&amp;rsquo;s next?&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m torn right now. I want to understand this material better, but I wonder if getting into lower-level, specific material might help me understand AI and machine learning better. What will likely happen is that I&amp;rsquo;ll copy and paste this content into Claude.ai and suggest a path forward for me.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;update-2025-02-17&#34;&gt;Update: 2025-02-17&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/sebastianraschka/&#34;&gt;Sebastian Raschka&lt;/a&gt; sent me a kind message in response to this post and clarified some of my thinking. To quote him:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;Every time it does a loop like this, it tokenizes the entire input (or up to a limit, known as a context limit or context window)&amp;rdquo;. You do this initially when you parse the input text. But then you technically don&amp;rsquo;t need to re-tokenize anything. You can leave the generated output in the tokenized form when creating the next token.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What I mean is if the text is&lt;/p&gt;
&lt;p&gt;&amp;ldquo;My cat saw a mouse&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The tokens might be &amp;ldquo;123 1 5 6 99&amp;rdquo; (numbers are arbitrary examples). Then the LLM generates the token 801 for &amp;ldquo;jump&amp;rdquo;. Then you simply use &amp;ldquo;123 1 5 6 99 801&amp;rdquo; as the input for the next word.&lt;/p&gt;
&lt;p&gt;When you show the output to the user, then you convert back into text.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&amp;ldquo;Saving all those relationships between the tokens are known as the “weights” of the model.&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I would say that relationships between tokens are the attention scores. The model weights are more like values that are involved in computing things like the attention scores (and other things).&lt;/p&gt;
&lt;p&gt;Now that you finished the book, in case you are bored, I do also have some more materials as bonus material in the GitHub repository.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;d say the GPT-&amp;gt;Llama conversion (&lt;a href=&#34;https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/07_gpt_to_llama&#34;&gt;https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/07_gpt_to_llama&lt;/a&gt;) and the DPO preference tuning (&lt;a href=&#34;https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb&#34;&gt;https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb&lt;/a&gt;) are maybe the most interesting ones.&lt;/p&gt;
&lt;p&gt;I also just uploaded some PyTorch tips for increasing the training speed of the model: &lt;a href=&#34;https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/10_llm-training-speed&#34;&gt;https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/10_llm-training-speed&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;These materials are less polished than the book itself, but maybe you&amp;rsquo;ll still find them useful!&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>New Blog</title>
      <link>http://localhost:1313/posts/new-blog/</link>
      <pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/new-blog/</guid>
      <description>&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;Does the internet need another blog? Definitively, no. Do I have original insights that you will benefit from reading? Most likely not. So what&amp;rsquo;s the point of this blog?&lt;/p&gt;
&lt;p&gt;I recently changed jobs, and I&amp;rsquo;m learning a lot. I retain information better when I describe it to someone. I also have a fear that if I constantly regurgitate my ongoing education to my close family, they will eventually want to murder me. That&amp;rsquo;s where this blog comes in. I&amp;rsquo;m going to teach you what I&amp;rsquo;m learning, so I can learn it better.&lt;/p&gt;
&lt;p&gt;Inevitably, I&amp;rsquo;ll forget about this blog. Posts will become less frequent, and then stop completely. At some point, I&amp;rsquo;ll just stop writing here completely. After a while, I&amp;rsquo;ll find a new use for my domain name, and this will cease to exist, except in the ever growing dataset of archive.org. So, let&amp;rsquo;s get on with it!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>