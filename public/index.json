[{"content":"Building a large language model from scratch I\u0026rsquo;m a machine learning / A.I. hobbyist. The technologies fascinate me, and I can\u0026rsquo;t seem to learn enough about them. Sebastian Raschka\u0026rsquo;s book, Build a Large Language Model (From Scratch) caught my eye. I don\u0026rsquo;t recall how I stumbled on it, but I found it when it was still in early access from Manning Publications. I purchased it, and started working through it as the final chapters were being written and released. I just completed the book and all the included work and loved every minute of it.\nMy approach A while ago, I read some advice about learning programming from digital books and tutorials. The advice was to never copy and paste code from samples but to hand-type all the code. I took that approach with this book. I typed every single line of code (except for a couple of blocks which were highly repetitive and long). You can see all my work here: https://github.com/controversy187/build-a-large-language-model\nI did my best to work in section chunks. I didn\u0026rsquo;t want to start a section unless I had the time dedicated to completing it. Some sections are pretty short, others are fairly involved and time-consuming.\nI built this in Jupyter Notebooks on my laptop, which is pretty underpowered for this type of work. The premise of the book was that you can build an LLM on consumer hardware, and it can perform decently well. As I\u0026rsquo;m writing this, I\u0026rsquo;m currently fine-tuning my model locally. My model is about 50 steps into a 230-step tuning, and I just crossed the 20-minute execution time mark. The earlier code samples ran quicker, but the last few sections used larger models, which slowed things down considerably.\nI didn\u0026rsquo;t do most of the supplemental exercises. I tend to have an \u0026ldquo;I want to do ALL THE THINGS!\u0026rdquo; personality. The drawback is that if I take the time to do all the things, I eventually get long-term distracted and never actually finish what I started. So I sort of rushed through this book. I even took several weeks off around Christmas and New Year\u0026rsquo;s. But I got back into it and powered through the last few chapters.\nSo, more or less, I read through the chapters and wrote all the mandatory coding assignments.\nLearnings What can I tell you about large language models? A lot more than I could before I started this book, but certainly not all the things the author attempted to teach me. I\u0026rsquo;ll summarize my understanding, but I could be wrong about some of these things, and I most certainly forgot or misunderstood others.\nTokenization \u0026amp; Vocabulary A large language model starts its life by building a vocabulary of text. A massive amount of text is distilled down into a list of unique words. Each word is then translated into an integer because computers like numbers more than they like words. This process is referred to as \u0026ldquo;tokenization\u0026rdquo;, where the word is replaced with a numerical token. So now we have a list of unique tokens, which is the vocabulary of the large language model.\n# Build a more advanced tokenizer text = \u0026#34;Hello, world. Is this-- a test?\u0026#34; result = re.split(r\u0026#39;([,.:;?_!\u0026#34;()\\\u0026#39;]|--|\\s)\u0026#39;, text) result = [item.strip() for item in result if item.strip()] print(result) # Outputs \u0026#34;[\u0026#39;Hello\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;world\u0026#39;, \u0026#39;.\u0026#39;, \u0026#39;Is\u0026#39;, \u0026#39;this\u0026#39;, \u0026#39;--\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;test\u0026#39;, \u0026#39;?\u0026#39;]\u0026#34; all_words = sorted(set(result)) vocab_size = len(all_words) print(vocab_size) # Outputs 10 # Display the first 51 tokens in our vocabulary. vocab = {token:integer for integer,token in enumerate(all_words)} for i, item in enumerate(vocab.items()): print(item) # Outputs: (\u0026#39;,\u0026#39;, 0) (\u0026#39;--\u0026#39;, 1) (\u0026#39;.\u0026#39;, 2) (\u0026#39;?\u0026#39;, 3) (\u0026#39;Hello\u0026#39;, 4) (\u0026#39;Is\u0026#39;, 5) (\u0026#39;a\u0026#39;, 6) (\u0026#39;test\u0026#39;, 7) (\u0026#39;this\u0026#39;, 8) (\u0026#39;world\u0026#39;, 9) # In this example, the id 9 represents the word \u0026#34;world\u0026#34;. 5 represents \u0026#34;Is\u0026#34;. etc. This is where my understanding gets fuzzy. We didn\u0026rsquo;t get very far before that happened, \u0026rsquo;eh? Now, we take that massive amount of text we were using earlier to create the vocabulary (or a subset, or totally different text), and we tokenize the entire text. We do this by using the vocabulary we built previously and substituting the words in the training text for their equivalent token value. This is now our training text.\nModel Training \u0026amp; Relationships With that complete, we can \u0026ldquo;train\u0026rdquo; the model. This process involves taking each token in the vocabulary and building a relationship to each other token in the vocabulary, based on those tokens\u0026rsquo; relative positions to each other in the training text. So if the word \u0026ldquo;cat\u0026rdquo; is followed by the word \u0026ldquo;jump\u0026rdquo;, the model records that relationship. But it also records the relationship of the word \u0026ldquo;cat\u0026rdquo; to other words in the text. So \u0026ldquo;jump\u0026rdquo; follows \u0026ldquo;cat\u0026rdquo;, but maybe it does so more frequently when they are close to the word \u0026ldquo;mouse\u0026rdquo;. And maybe less frequently when they are close to the word \u0026ldquo;nap\u0026rdquo;. Recording ALL these relationships would require a massive dataset, so the relationships are mathematically reduced and approximated. There are definitely more technical terms to use, and the book went into them. I definitely forget them, though.\nText Generation Process Now, if you provide a starter text to the model, it will try to complete the text for you. Continuing our example, if I gave the model the text \u0026ldquo;My cat saw a mouse and it\u0026rdquo;, based on the word cat being close to the word mouse, it might predict the word \u0026ldquo;jumped\u0026rdquo; to come next. So it appends the word \u0026ldquo;jumped\u0026rdquo; to the text I submitted, and then it takes that whole new sentence and feeds it back into itself. So now the input text is \u0026ldquo;My cat saw a mouse and it jumped\u0026rdquo;. The next output word could be \u0026ldquo;on\u0026rdquo;, so it appends that word and feeds this concatenated output back into its input.\nEvery time it does a loop like this, it tokenizes the entire input (or up to a limit, known as a context limit or context window) and then calculates the most likely next token, then converts it all back to text for us to read.\nModel Weights \u0026amp; Distribution Saving all those relationships between the tokens are known as the \u0026ldquo;weights\u0026rdquo; of the model. Those can be distributed, so if you train a model on a given training text, you can give that to your friends and they can use those model weights to predict text similar to that training text.\nFine Tuning Fine-tuning is the process of training a model for specific\u0026hellip; things. My mind is getting fuzzier here, so I\u0026rsquo;m not going to go into this deeper. Suffice it to say, that you start with a base language model and continue to train it using specific input and output pairs. In the book, we built a spam classifier that determined if a given message was spam or not, as well as a model that will follow instructions. That\u0026rsquo;s actually the one that\u0026rsquo;s being trained right now as I write this post, so I\u0026rsquo;m not sure how it will turn out. Based on the fact that it\u0026rsquo;s published in a book, I think it will come out just fine.\nSo while I\u0026rsquo;m not completely done with the book, I\u0026rsquo;m very nearly there. I did learn a lot of great concepts, although obviously some of them weren\u0026rsquo;t retained. It would probably behoove me to go back through the book again and quickly breeze through it, in order to refresh my memory and cement my learnings.\nMeta learnings Other than the technical aspects of Large Language Models, what else did I learn through this experience?\nThrough my experiment with typing all the code samples by hand, I can say that my time would have been better spent with a different approach. If I do this again, I\u0026rsquo;ll probably not type all the code snippets, but rather \u0026ldquo;type\u0026rdquo; them in my mind, and really understand what each line does. The times I learned the most were actually when I made a typo and had to go back through my code to debug it. That forced me to understand what was happening so I could figure out what went wrong.\nI learn better with paper, rather than a digital book. I don\u0026rsquo;t know why. I had both available to me, and I read the first couple of chapters in the paper book. That information stuck better. Maybe because it was earlier in the book and simpler to understand, or maybe the format played into it. But I enjoyed it better, regardless.\nI didn\u0026rsquo;t have to \u0026ldquo;figure out\u0026rdquo; anything, and I think that hampered my learning. There are supplemental exercises in the book, where the author gives you a problem and you have to figure out how to solve it. The answers are given in his GitHub repository. That would have slowed me down a lot, but I\u0026rsquo;m very confident that I would have learned the material better.\nWhat\u0026rsquo;s next? I\u0026rsquo;m torn right now. I want to understand this material better, but I wonder if getting into lower-level, specific material might help me understand AI and machine learning better. What will likely happen is that I\u0026rsquo;ll copy and paste this content into Claude.ai and suggest a path forward for me.\n","permalink":"http://localhost:1313/posts/build-a-large-language-model/","summary":"Building a large language model from scratch I\u0026rsquo;m a machine learning / A.I. hobbyist. The technologies fascinate me, and I can\u0026rsquo;t seem to learn enough about them. Sebastian Raschka\u0026rsquo;s book, Build a Large Language Model (From Scratch) caught my eye. I don\u0026rsquo;t recall how I stumbled on it, but I found it when it was still in early access from Manning Publications. I purchased it, and started working through it as the final chapters were being written and released.","title":"Build a Large Language Model From Scratch"},{"content":"I\u0026rsquo;m a technologist who\u0026rsquo;s followed my curiosity through various roles in software development, agile leadership, and now data analytics. This blog documents my ongoing journey of learning and exploration in technology.\nProfessional Background I got my start in computers by hacking around on a Commodore 64. I distinctly remember loading an ASCII bowling game, and finding out that I could see the source code before running the program. Not knowing what source code was at the time, I simply saw the word \u0026ldquo;BOWLING\u0026rdquo;, as it appeared on the title screen. Being in elementary school at the time, I changed it to \u0026ldquo;POOPING\u0026rdquo;, ran the program, saw my changes, and my life was set on a very specific course.\nFrom these humble beginnings, I went on to study computer networking through college. Upon graduating, I immediately began a career in web development, because CISCO certifications are hard. Early career HTML, CSS, JS, Classic ASP and PHP got me started in the world of software development. As technologies evolved, I barely did, continuing my skillset and going moderately deep in the PHP and WordPress world. I dabbled in AWS architecture in the early AWS days, and eventually found more joy at the intersection of people and technology than I did staying heads down at the keyboard.\nThose interests led me into the world of Agile, and I became a Scrum Master. I loved working with a small focused team, and we did a lot of things that weren\u0026rsquo;t really Scrum, but worked well for us. I ended up drinking the Scaled Agile Framework Cool-Aid, and even did a little SAFe consulting. This led me to Portfolio Manager role.\nTrying to scale Agile was a frustrating experience. I wasn\u0026rsquo;t close to the work, and I didn\u0026rsquo;t feel like I was providing value. In my free time, I was learning about machine learning, artificial intelligence, blockchain technologies, and other technological interests. My passions didn\u0026rsquo;t lie with the Scaled Agile Framework.\nDisillusioned with SAFe, I found an opportunity to be a product manager for a data analytics team. I took a risk and pursued it, and the manager took a risk on me and hired me. That\u0026rsquo;s where our story begins, and the impetus for this website. I\u0026rsquo;m learning a lot, and I want to document what I\u0026rsquo;m learning.\nPersonal Background I spend the bulk of my days in front of a computer. When I\u0026rsquo;m not here, I like to be outside. I enjoy skiing, ultimate frisbee, Brazilian Jiu Jitsu, kiteboarding, hiking, traveling, and pretty much anything active. I like to do these things with family as much as possible, and I also enjoy amateur videography. That allows me to capture memories in a compelling way that I can enjoy with my wife and kids, and extended family.\n","permalink":"http://localhost:1313/about/","summary":"About Brett Fitzgerald","title":"About Me"},{"content":"Why? Does the internet need another blog? Definitively, no. Do I have original insights that you will benefit from reading? Most likely not. So what\u0026rsquo;s the point of this blog?\nI recently changed jobs, and I\u0026rsquo;m learning a lot. I retain information better when I describe it to someone. I also have a fear that if I constantly regurgitate my ongoing education to my close family, they will eventually want to murder me. That\u0026rsquo;s where this blog comes in. I\u0026rsquo;m going to teach you what I\u0026rsquo;m learning, so I can learn it better.\nInevitably, I\u0026rsquo;ll forget about this blog. Posts will become less frequent, and then stop completely. At some point, I\u0026rsquo;ll just stop writing here completely. After a while, I\u0026rsquo;ll find a new use for my domain name, and this will cease to exist, except in the ever growing dataset of archive.org. So, let\u0026rsquo;s get on with it!\n","permalink":"http://localhost:1313/posts/new-blog/","summary":"Why? Does the internet need another blog? Definitively, no. Do I have original insights that you will benefit from reading? Most likely not. So what\u0026rsquo;s the point of this blog?\nI recently changed jobs, and I\u0026rsquo;m learning a lot. I retain information better when I describe it to someone. I also have a fear that if I constantly regurgitate my ongoing education to my close family, they will eventually want to murder me.","title":"New Blog"}]