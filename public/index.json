[{"content":"My Journey with MCP Servers I recently learned of MCP (Model Control Protocol) servers through a post on Hacker News. The premise seems really neat. MCP is essentially a protocol that allows AI models like Claude to interact with external tools and services through a standardized interface. I can write a very simple server and create \u0026ldquo;tools\u0026rdquo; for the Claude.ai desktop application to connect to and use. The example in the post created some tools that gave Claude access to read and write to the local filesystem (unrestricted, by the way). The original intent was to have Claude write some application. I cloned and ran the sample MCP server from the article to play with it, and was immediately impressed. This unlocks so many opportunities to integrate an LLM with countless services!\nHold on to that thought\u0026hellip;\nMy Second Brain For the past several years, I\u0026rsquo;ve been using Tiago Forte\u0026rsquo;s Build a Second Brain method using Obsidian.md for my second brain. I implemented the PARA Method for organization, and create a Daily Note every day to link my chronological life to my disorganized brain. It\u0026rsquo;s been a good system, even if I don\u0026rsquo;t use it to it\u0026rsquo;s fullest. At the very least, I have all my notes synced with me between my desktop, laptop, and phone, thanks to Syncthing. That being said, Obsidian\u0026rsquo;s search capabilities are relatively lackluster.\nThis is where my MCP server can help.\nGetting things going To get it all setup, I basically cloned the MCP server linked above, and then set it up as they described, creating the server config in the Claude Desktop app. Since I\u0026rsquo;m running Linux, I had to hack around a bit to get the Windows Claude client running, but it works fine. With the MCP server connected, I now showed that I had several tools available to Claude.\nAfter some sample queries, I validated that Claude now had full access to my filesystem!\nPlumbing the depths of my second brain\u0026hellip; Since I write a Daily Note every day, I first asked Claude to summarize my daily notes for the month of March 2025. It came up with accurate and surprisingly detailed insights.\nMarch 2025 Daily Notes Summary Personal Life Family Activities: Started the month at ****** with the ****** and ****** families, though you noted it was chaotic with misaligned schedules Went skiing with ****** at Crystal Mountain on March 10th, where you had a meaningful father-son conversation about life and relationships Your daughter ****** attended Middle School Ministry several times References to your family (******, ******, ******, ******) appear consistently throughout the month Fitness: Maintained consistent workout schedule with 15 recorded workout days Primarily practiced Jiu-Jitsu at ****** (both gi and nogi classes) Also incorporated running into your fitness routine Tracked your weight throughout the month, ranging from 179 lbs to 187 lbs, with an average around 182 lbs Spiritual Practice: Maintained daily quiet time starting mid-month (March 18-31) Studied the Gospel of Matthew, progressing sequentially from Matthew 12:38 through Matthew 14:33 Each quiet time session appears well-documented with dedicated notes Professional Life Regular Meetings: ****** (almost daily) Weekly kickoff meetings (Mondays) One-on-one meetings with various team members including ******, ******, and ****** Key Projects: ****** Rush Orders project with weekly check-ins ****** project with multiple coordination meetings Corporate Injury Analysis work Supply Chain ****** activities and meetings Non******stock ETA modeling work (created ******) Professional Development: Participated in Looker Studio training for data visualization Worked on Google Maps Optimization POC Set up Gemini in VSCode Explored Jira metrics via API People Management: Reached out to ****** about a ****** Provided feedback on ******\u0026#39;s Flow Metrics presentation Worked with various team members including ******, ******, ******, and ****** Your month shows a balanced life with consistent attention to physical fitness, spiritual growth, family time, and professional responsibilities, with a particular focus on data and supply chain projects at work. This was great! For years, my Second Brain has been a storage device. I had the ability to go back and manually retrieve that information, but I had to read the notes into my short-term (actual) brain, and hold it there while I search for other information to connect it to. Obsidian and the Second Brain method make this simpler by encouraging the use of internal links in notes, but now I had another tool for synthesizing meaningful information from all these connections!\nCreating Notes Equipped with a tool for analyzing my second brain, I turned toward project management. I\u0026rsquo;ve had a few ideas for side projects that I wanted to flesh out a bit. I\u0026rsquo;ve worked with LLMs in the past to bounce ideas off and refine some thoughts, but now I could generate documentation around these ideas, store that information, and retrieve it later for ongoing processing and development! I took a conversation about a project I\u0026rsquo;m thinking through, described my desired goal, and then asked Claude to develop a plan for a low-code, low-cost MVP to test market fit. It described the approach, I refined it a bit, and then asked Claude to store the project documentation in my Obsidian Vault. Boom, project plan and action steps created!\nReflection MCP Servers seem extremely powerful. For such a long time, LLMs have seemed contained to limited use cases. Plugins for code editors have allowed functionality in coding and chatbots are common. Further integrations have required the use of APIs and coding to leverage the power of LLMs in other contexts. Now, with MCP Servers, it really feels like we have simple-to-create interfaces that allow LLMs to interact with the rest of the digital world. What will you create?\n","permalink":"http://localhost:1313/posts/mcp-server-experiences/","summary":"My Journey with MCP Servers I recently learned of MCP (Model Control Protocol) servers through a post on Hacker News. The premise seems really neat. MCP is essentially a protocol that allows AI models like Claude to interact with external tools and services through a standardized interface. I can write a very simple server and create \u0026ldquo;tools\u0026rdquo; for the Claude.ai desktop application to connect to and use. The example in the post created some tools that gave Claude access to read and write to the local filesystem (unrestricted, by the way).","title":"Obsidian, MCP Servers, and Supercharging Your Second Brain with AI"},{"content":"Controlling clip speed in DaVinci Resolve This is the process I just learned for adjusting speed within clips in DaVinci Resolve 19. This isn\u0026rsquo;t an adjustment for an entire clip, but portions of a clip. You can see the use-case in several of the shots here. In the past, I’d cut a clip where I wanted the speed to change and adjust the middle section’s speed, turning one clip into three.\nAdjust clip speed using Time Controls I\u0026rsquo;ll use the last clip from the above video as a tutorial. The first step is to select the clip you want to adjust on your timeline, in the Edit tab. Right-click the clip and select \u0026ldquo;Retime Controls\u0026rdquo; (ctrl+R on Windows). You’ll see blue triangles pointing right. Move your playhead (the timeline cursor showing your current frame) to the frame where you want the speed change to start. At the bottom of the clip on the timeline, you should see an indicator that shows \u0026ldquo;100%\u0026rdquo; followed by a downward facing triangle. Clicking that triangle pops open a context menu. From there, select \u0026ldquo;Add Speed Point.\u0026rdquo; This functions like a keyframe, if you\u0026rsquo;ve done other video effects or editing in the past. With that divider there, you can see the clip is effectively bisected at the frame you have selected, and each side shows 100%. Now, you can select either side (I\u0026rsquo;ll use the right-side) to adjust the speed. Clicking the black down arrow there allows you \u0026ldquo;Change Speed\u0026rdquo; to a handful of preset options. I\u0026rsquo;ll select 25%. Now the arrows change color and extend. Effectively, you have created another clip, with a slower speed. To resume normal speed later, move your playhead to that spot and add another speed point (via the black arrow). Then reset the right side to 100%. Your final result is a clip that runs at normal speed, slows down in the middle, and resumes normal speed at the end! ","permalink":"http://localhost:1313/posts/speed-controls-in-davinci-resolve/","summary":"Controlling clip speed in DaVinci Resolve This is the process I just learned for adjusting speed within clips in DaVinci Resolve 19. This isn\u0026rsquo;t an adjustment for an entire clip, but portions of a clip. You can see the use-case in several of the shots here. In the past, I’d cut a clip where I wanted the speed to change and adjust the middle section’s speed, turning one clip into three.","title":"Speed Controls in Davinci Resolve"},{"content":"A videography hobbyist In my free time, I like shooting videos of my family\u0026rsquo;s adventures and doing some basic editing. I shoot on my cellphone and a GoPro. For the past several years, I have rendered my final projects in 1080p at 24 frames per second. I liked the ability to shoot in 4k and still \u0026ldquo;zoom in\u0026rdquo; digitally to 1080. That also let me shoot slow motion video at 1080, and match my final render resolution. I recently got a newer GoPro, so now I can shoot 4k at 120 fps, which allows me slow down to 20% speed if I render my final project at 24 fps.\nWith this advent of being able to shoot slow motion in 4k resolution, I decided to start rendering my projects in 4k, by default. I\u0026rsquo;m also experimenting with doing very quick edits, just splicing the day\u0026rsquo;s footage together, applying automatic color balancing per-clip, and then rendering at 60fps. This is more of a \u0026ldquo;home movie\u0026rdquo; of a day or event, rather than a curated, edited highlight video. I do all this in Davinci Resolve. Previously, I would be able to render my 1080 videos at 24fps and be happy with the file size and quality of picture. Now that I am rendering at four times the resolution and two and a half times the framerate, my output filesize has ballooned, and I need to pay better attention to my compression.\nI want to find a good balance between output filesize and quality for my home movies.\nComparison of projects In the past, I would shoot for around 100 megabytes per minute of rendered video. So a 5 minute video, rendered at a resolution of 1080, at 24 frames per second would come in around 500 MB. For videos I really spent time on, I would bump up my quality settings and I\u0026rsquo;d be happy with a 3 gig file for a 5 minute video.\nI recently rendered a video using Davinci Resolve\u0026rsquo;s 4k \u0026ldquo;Master\u0026rdquo; preset. So a resolution of 4k, at 60 frames per second, and a duration of 22 minutes came in at a whopping 75 gigabytes (~3.4 GB / min). I used Resolve\u0026rsquo;s \u0026ldquo;YouTube\u0026rdquo; preset, and that reduced the filesize to 1.8 GB.(~80 MB / min). That is a significant difference!\nFor reference, my input files, shot on the GoPro Hero 13 were shot in 4k, mostly at 120 fps. They totaled 18.2GB, so my rendered file was actually four times larger than my source material!\nThe two questions I have are:\nWhat is the difference in output files between these two? Is there a noticeable difference in visible quality? Differences in objective data I wrote a python script that compares various aspects of the videos. I also ran them through the various presets in Handbrake to see how they compare. The video is 21:49 long. These are the results of that comparison, in order of increasing filesize.\nFilename Bitrate (kbps) Resolution Framerate (FPS) Video Codec Filesize Source Video (Sample).MP4 120000 3840x2160 119.88 hevc 67 MB Resolve - YouTube - h264.mp4 11618 3840x2160 60 h264 1.8 GB Resolve - YouTube - h265.mp4 10566 3840x2160 60 hevc 1.7 GB Resolve - Master - h264 - HandBrake - Fast.mp4 37948 3840x2160 60 hevc 6 GB Resolve - Master - h264 - HandBrake - VeryFast.mp4 41025 3840x2160 60 hevc 6.5 GB Resolve - Master - h264 - HandBrake - HQ.mp4 57001 3840x2160 60 hevc 9 GB Resolve - Master - h264 - HandBrake - Super HQ.mp4 78210 3840x2160 60 hevc 12.5 GB Resolve - Master - h265.mp4 473927 3840x2160 60 h264 75.7 GB Resolve - Master - h264.mp4 474208 3840x2160 60 h264 75.8 GB Obviously, Handbrake is going a good job at compressing the video, and lowering the bitrate, thus reducing filesize. But how do the videos actually look?\nSubjective comparison Here are images captured from each of the above videos.\nDavinci Resolve, YouTube preset, h264 Davinci Resolve, YouTube Preset, h265 Handbrake, Fast Handbrake, Very Fast Handbrake, High Quality Handbrake, Super High Quality Davinci Resolve, Master - h264 Davinci Resolve, Master - h265 In these very specific examples, you can immediately see that the DaVinci Resolve YouTube presets are not good. The h264 version shows artifiacts on the right side of the frame and all detail in the snow on the ground is completely lost. Interestingly, the h265 codec doesn\u0026rsquo;t lose as much detail, and is slightly smaller. In the case were you need a small file, it seems like the h265 does a better job at these lower bitrates.\nWhen we jump up into the Handbrake re-encodes, things get noticeably better. Honestly, from the still frames it\u0026rsquo;s very hard (for me) to tell the difference between these images, all the way through to the masters. Even when I watch the playback of the videos themselves, I\u0026rsquo;m hard pressed to see any difference. It could be that the source clips themselves only have a 120Mbps bitrate, and we\u0026rsquo;re re-encoding at a higher bitrate for the masters (474Mbps).\nConclusions To really judge this fairly, I probably should have rendered everything out from DaVinci Resolve and manually adjusted the bitrate. Based on these tests, though, I\u0026rsquo;m not seeing a noticeable loss in quality between the 474Mbps best quality from Resolve and a re-encoded to 78Mbps in Handbrake. For the time being, I\u0026rsquo;m planning to render out from Resolve and limiting my bitrate to 80Mbps. That\u0026rsquo;s only 590 MB or so per minute of video, which isn\u0026rsquo;t bad for what I\u0026rsquo;m doing with them.\n","permalink":"http://localhost:1313/posts/video-compression-analysis/","summary":"A videography hobbyist In my free time, I like shooting videos of my family\u0026rsquo;s adventures and doing some basic editing. I shoot on my cellphone and a GoPro. For the past several years, I have rendered my final projects in 1080p at 24 frames per second. I liked the ability to shoot in 4k and still \u0026ldquo;zoom in\u0026rdquo; digitally to 1080. That also let me shoot slow motion video at 1080, and match my final render resolution.","title":"Video Compression Analysis"},{"content":"Making Music Videos I am very formulaic in how I make videos for my personal library.\nShoot some footage on my cell phone and/or a GoPro. Find a song that captures the feeling of the event. Import all the clips from the event to DaVinci Resolve, and the song selected above. Add the song to the timeline. I then go through the clips, usually chronologically, and see which ones are \u0026ldquo;cool enough\u0026rdquo; to make it the cut. If I end up with more clips than I have time for, ruthlessly cut clips until I have the right amount for the song. Manually drag clip start and end points to align with the measures of the song. Apply Autolevels to each clip, based on the midpoint of the clip (I\u0026rsquo;m red/green color blind and haven\u0026rsquo;t invest any time into figuring out how to color grade). Export and Upload. It\u0026rsquo;s time consuming, but I usually enjoy it. One part that is always frustrating, though, is aligning the cuts in the clips to the measures in the song. I play the timeline and tap the beat on my desk, usually counting out the beats. If I\u0026rsquo;m lucky, there is a visible spike in the waveform on the beats, so I can drag my video clip boundaries to align with them. Sometimes I just have to guess where they are. The whole point is that a generic video like this will draw someone in mre effectively if the video is synchronized with the audio. It\u0026rsquo;s hard to describe, but without the synchronization, the video just sort of plays and the music creates a general feeling. When the timings line up, it feels like the music is pushing the visuals forward, creating more of an emotional resonance with the viewer. That\u0026rsquo;s my experience, at least.\nI wanted to figure out how I could make it easier to find those beats in the audio, and the measure boundaries, so I could speed my workflow.\nMaking a click-track My first attempt was to use python to analyize the audio file of the song I was using. I figured that there is likely some library out there that would determine the beats and hopefully the measures. I could then use that information to generate a new audio file. A single \u0026ldquo;click\u0026rdquo; sound on the beats, and a higher pitched click on the first beat of each measure. With that audio file, I could add it to the timeline in Resolve, mute it, and have a very clear visual to align my video clips to.\nWith some help from Claude.ai and ChatGPT, I played with the librosa libary. It was suprisingly accurate at finding the beats and generating a click track. The biggest hurdle I ran into happened during musical transitions. It seems that librosa analyizes the music for acoustic \u0026ldquo;density\u0026rdquo;. Music is generally more \u0026ldquo;dense\u0026rdquo; on the beats, which is where louder drums are hit, guitar chords are strummed, keys are struck, and generally most or all of the instruments \u0026ldquo;happen\u0026rdquo; at the same time.\nFor example, a guitar chord would be played on a beat, and then it is left to resonate until the next beat, or possibly a divison of the next beat. So with most of the instruments striking on the beats, those moments are more dense. Any given song will usually have a relatively consistent feel, and it seems like librosa does a good job of determining that feel, finding those dense moments and aligning them to the beats. Where it struggled for me is when the general density of the music changed.\nI was testing with The Allman Brother\u0026rsquo;s song Ramblin\u0026rsquo; Man. This song starts with a clearly defined introduction which is mainly electric guitar for a couple of measures, and then the full band comes in. The problem is that librosa seems to normalize to the relatively low density of the electric guitar-only intro, and correctly identifies the beats. But once the full band comes in, the density goes up significantly, and librosa identifies extra beats for a short period, until it renormalizes to the higher density of the song. That throws off the counts of our beats, and then our measures are no longer in alignment.\nI played with several parameters of librosa, but never got to satisfactory results.\nMaking project markers DaVinci Resolve (and most other populary video editing tools) support a feature called Markers. I haven\u0026rsquo;t used them before, so this is my first foray into them. The idea is that you can set specific markers on your timeline and then snap clips to them.\n","permalink":"http://localhost:1313/posts/automatic-music-markers-in-davinci-resolve/","summary":"Making Music Videos I am very formulaic in how I make videos for my personal library.\nShoot some footage on my cell phone and/or a GoPro. Find a song that captures the feeling of the event. Import all the clips from the event to DaVinci Resolve, and the song selected above. Add the song to the timeline. I then go through the clips, usually chronologically, and see which ones are \u0026ldquo;cool enough\u0026rdquo; to make it the cut.","title":"Automatic Music Markers in Davinci Resolve"},{"content":"Building a large language model from scratch I\u0026rsquo;m a machine learning / A.I. hobbyist. The technologies fascinate me, and I can\u0026rsquo;t seem to learn enough about them. Sebastian Raschka\u0026rsquo;s book, Build a Large Language Model (From Scratch) caught my eye. I don\u0026rsquo;t recall how I stumbled on it, but I found it when it was still in early access from Manning Publications. I purchased it, and started working through it as the final chapters were being written and released. I just completed the book and all the included work and loved every minute of it.\nMy approach A while ago, I read some advice about learning programming from digital books and tutorials. The advice was to never copy and paste code from samples but to hand-type all the code. I took that approach with this book. I typed every single line of code (except for a couple of blocks which were highly repetitive and long). You can see all my work here: https://github.com/controversy187/build-a-large-language-model\nI did my best to work in section chunks. I didn\u0026rsquo;t want to start a section unless I had the time dedicated to completing it. Some sections are pretty short, others are fairly involved and time-consuming.\nI built this in Jupyter Notebooks on my laptop, which is pretty underpowered for this type of work. The premise of the book was that you can build an LLM on consumer hardware, and it can perform decently well. As I\u0026rsquo;m writing this, I\u0026rsquo;m currently fine-tuning my model locally. My model is about 50 steps into a 230-step tuning, and I just crossed the 20-minute execution time mark. The earlier code samples ran quicker, but the last few sections used larger models, which slowed things down considerably.\nI didn\u0026rsquo;t do most of the supplemental exercises. I tend to have an \u0026ldquo;I want to do ALL THE THINGS!\u0026rdquo; personality. The drawback is that if I take the time to do all the things, I eventually get long-term distracted and never actually finish what I started. So I sort of rushed through this book. I even took several weeks off around Christmas and New Year\u0026rsquo;s. But I got back into it and powered through the last few chapters.\nSo, more or less, I read through the chapters and wrote all the mandatory coding assignments.\nLearnings What can I tell you about large language models? A lot more than I could before I started this book, but certainly not all the things the author attempted to teach me. I\u0026rsquo;ll summarize my understanding, but I could be wrong about some of these things, and I most certainly forgot or misunderstood others.\nTokenization \u0026amp; Vocabulary A large language model starts its life by building a vocabulary of text. A massive amount of text is distilled down into a list of unique words. Each word is then translated into an integer because computers like numbers more than they like words. This process is referred to as \u0026ldquo;tokenization\u0026rdquo;, where the word is replaced with a numerical token. So now we have a list of unique tokens, which is the vocabulary of the large language model.\n# Build a more advanced tokenizer text = \u0026#34;Hello, world. Is this-- a test?\u0026#34; result = re.split(r\u0026#39;([,.:;?_!\u0026#34;()\\\u0026#39;]|--|\\s)\u0026#39;, text) result = [item.strip() for item in result if item.strip()] print(result) # Outputs \u0026#34;[\u0026#39;Hello\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;world\u0026#39;, \u0026#39;.\u0026#39;, \u0026#39;Is\u0026#39;, \u0026#39;this\u0026#39;, \u0026#39;--\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;test\u0026#39;, \u0026#39;?\u0026#39;]\u0026#34; all_words = sorted(set(result)) vocab_size = len(all_words) print(vocab_size) # Outputs 10 # Display the first 51 tokens in our vocabulary. vocab = {token:integer for integer,token in enumerate(all_words)} for i, item in enumerate(vocab.items()): print(item) # Outputs: (\u0026#39;,\u0026#39;, 0) (\u0026#39;--\u0026#39;, 1) (\u0026#39;.\u0026#39;, 2) (\u0026#39;?\u0026#39;, 3) (\u0026#39;Hello\u0026#39;, 4) (\u0026#39;Is\u0026#39;, 5) (\u0026#39;a\u0026#39;, 6) (\u0026#39;test\u0026#39;, 7) (\u0026#39;this\u0026#39;, 8) (\u0026#39;world\u0026#39;, 9) # In this example, the id 9 represents the word \u0026#34;world\u0026#34;. 5 represents \u0026#34;Is\u0026#34;. etc. This is where my understanding gets fuzzy. We didn\u0026rsquo;t get very far before that happened, \u0026rsquo;eh? Now, we take that massive amount of text we were using earlier to create the vocabulary (or a subset, or totally different text), and we tokenize the entire text. We do this by using the vocabulary we built previously and substituting the words in the training text for their equivalent token value. This is now our training text.\nModel Training \u0026amp; Relationships With that complete, we can \u0026ldquo;train\u0026rdquo; the model. This process involves taking each token in the vocabulary and building a relationship to each other token in the vocabulary, based on those tokens\u0026rsquo; relative positions to each other in the training text. So if the word \u0026ldquo;cat\u0026rdquo; is followed by the word \u0026ldquo;jump\u0026rdquo;, the model records that relationship. But it also records the relationship of the word \u0026ldquo;cat\u0026rdquo; to other words in the text. So \u0026ldquo;jump\u0026rdquo; follows \u0026ldquo;cat\u0026rdquo;, but maybe it does so more frequently when they are close to the word \u0026ldquo;mouse\u0026rdquo;. And maybe less frequently when they are close to the word \u0026ldquo;nap\u0026rdquo;. Recording ALL these relationships would require a massive dataset, so the relationships are mathematically reduced and approximated. There are definitely more technical terms to use, and the book went into them. I definitely forget them, though.\nText Generation Process Now, if you provide a starter text to the model, it will try to complete the text for you. Continuing our example, if I gave the model the text \u0026ldquo;My cat saw a mouse and it\u0026rdquo;, based on the word cat being close to the word mouse, it might predict the word \u0026ldquo;jumped\u0026rdquo; to come next. So it appends the word \u0026ldquo;jumped\u0026rdquo; to the text I submitted, and then it takes that whole new sentence and feeds it back into itself. So now the input text is \u0026ldquo;My cat saw a mouse and it jumped\u0026rdquo;. The next output word could be \u0026ldquo;on\u0026rdquo;, so it appends that word and feeds this concatenated output back into its input.\nEvery time it does a loop like this, it tokenizes the entire input (or up to a limit, known as a context limit or context window) and then calculates the most likely next token, then converts it all back to text for us to read. See update\nModel Weights \u0026amp; Distribution Saving all those relationships between the tokens are known as the \u0026ldquo;weights\u0026rdquo; of the model. See update Those can be distributed, so if you train a model on a given training text, you can give that to your friends and they can use those model weights to predict text similar to that training text.\nFine Tuning Fine-tuning is the process of training a model for specific\u0026hellip; things. My mind is getting fuzzier here, so I\u0026rsquo;m not going to go into this deeper. Suffice it to say, that you start with a base language model and continue to train it using specific input and output pairs. In the book, we built a spam classifier that determined if a given message was spam or not, as well as a model that will follow instructions. That\u0026rsquo;s actually the one that\u0026rsquo;s being trained right now as I write this post, so I\u0026rsquo;m not sure how it will turn out. Based on the fact that it\u0026rsquo;s published in a book, I think it will come out just fine.\nSo while I\u0026rsquo;m not completely done with the book, I\u0026rsquo;m very nearly there. I did learn a lot of great concepts, although obviously some of them weren\u0026rsquo;t retained. It would probably behoove me to go back through the book again and quickly breeze through it, in order to refresh my memory and cement my learnings.\nMeta learnings Other than the technical aspects of Large Language Models, what else did I learn through this experience?\nThrough my experiment with typing all the code samples by hand, I can say that my time would have been better spent with a different approach. If I do this again, I\u0026rsquo;ll probably not type all the code snippets, but rather \u0026ldquo;type\u0026rdquo; them in my mind, and really understand what each line does. The times I learned the most were actually when I made a typo and had to go back through my code to debug it. That forced me to understand what was happening so I could figure out what went wrong.\nI learn better with paper, rather than a digital book. I don\u0026rsquo;t know why. I had both available to me, and I read the first couple of chapters in the paper book. That information stuck better. Maybe because it was earlier in the book and simpler to understand, or maybe the format played into it. But I enjoyed it better, regardless.\nI didn\u0026rsquo;t have to \u0026ldquo;figure out\u0026rdquo; anything, and I think that hampered my learning. There are supplemental exercises in the book, where the author gives you a problem and you have to figure out how to solve it. The answers are given in his GitHub repository. That would have slowed me down a lot, but I\u0026rsquo;m very confident that I would have learned the material better.\nWhat\u0026rsquo;s next? I\u0026rsquo;m torn right now. I want to understand this material better, but I wonder if getting into lower-level, specific material might help me understand AI and machine learning better. What will likely happen is that I\u0026rsquo;ll copy and paste this content into Claude.ai and suggest a path forward for me.\nUpdate: 2025-02-17 Sebastian Raschka sent me a kind message in response to this post and clarified some of my thinking. To quote him:\n\u0026ldquo;Every time it does a loop like this, it tokenizes the entire input (or up to a limit, known as a context limit or context window)\u0026rdquo;. You do this initially when you parse the input text. But then you technically don\u0026rsquo;t need to re-tokenize anything. You can leave the generated output in the tokenized form when creating the next token. What I mean is if the text is\n\u0026ldquo;My cat saw a mouse\u0026rdquo;\nThe tokens might be \u0026ldquo;123 1 5 6 99\u0026rdquo; (numbers are arbitrary examples). Then the LLM generates the token 801 for \u0026ldquo;jump\u0026rdquo;. Then you simply use \u0026ldquo;123 1 5 6 99 801\u0026rdquo; as the input for the next word.\nWhen you show the output to the user, then you convert back into text.\n\u0026ldquo;Saving all those relationships between the tokens are known as the “weights” of the model.\u0026rdquo; I would say that relationships between tokens are the attention scores. The model weights are more like values that are involved in computing things like the attention scores (and other things).\nNow that you finished the book, in case you are bored, I do also have some more materials as bonus material in the GitHub repository.\nI\u0026rsquo;d say the GPT-\u0026gt;Llama conversion (https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/07_gpt_to_llama) and the DPO preference tuning (https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb) are maybe the most interesting ones.\nI also just uploaded some PyTorch tips for increasing the training speed of the model: https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/10_llm-training-speed\nThese materials are less polished than the book itself, but maybe you\u0026rsquo;ll still find them useful!\n","permalink":"http://localhost:1313/posts/build-a-large-language-model/","summary":"Building a large language model from scratch I\u0026rsquo;m a machine learning / A.I. hobbyist. The technologies fascinate me, and I can\u0026rsquo;t seem to learn enough about them. Sebastian Raschka\u0026rsquo;s book, Build a Large Language Model (From Scratch) caught my eye. I don\u0026rsquo;t recall how I stumbled on it, but I found it when it was still in early access from Manning Publications. I purchased it, and started working through it as the final chapters were being written and released.","title":"Build a Large Language Model From Scratch"},{"content":"I\u0026rsquo;m a technologist who\u0026rsquo;s followed my curiosity through various roles in software development, agile leadership, and now data analytics. This blog documents my ongoing journey of learning and exploration in technology.\nProfessional Background I got my start in computers by hacking around on a Commodore 64. I distinctly remember loading an ASCII bowling game, and finding out that I could see the source code before running the program. Not knowing what source code was at the time, I simply saw the word \u0026ldquo;BOWLING\u0026rdquo;, as it appeared on the title screen. Being in elementary school at the time, I changed it to \u0026ldquo;POOPING\u0026rdquo;, ran the program, saw my changes, and my life was set on a very specific course.\nFrom these humble beginnings, I went on to study computer networking through college. Upon graduating, I immediately began a career in web development, because CISCO certifications are hard. Early career HTML, CSS, JS, Classic ASP and PHP got me started in the world of software development. As technologies evolved, I barely did, continuing my skillset and going moderately deep in the PHP and WordPress world. I dabbled in AWS architecture in the early AWS days, and eventually found more joy at the intersection of people and technology than I did staying heads down at the keyboard.\nThose interests led me into the world of Agile, and I became a Scrum Master. I loved working with a small focused team, and we did a lot of things that weren\u0026rsquo;t really Scrum, but worked well for us. I ended up drinking the Scaled Agile Framework Cool-Aid, and even did a little SAFe consulting. This led me to Portfolio Manager role.\nTrying to scale Agile was a frustrating experience. I wasn\u0026rsquo;t close to the work, and I didn\u0026rsquo;t feel like I was providing value. In my free time, I was learning about machine learning, artificial intelligence, blockchain technologies, and other technological interests. My passions didn\u0026rsquo;t lie with the Scaled Agile Framework.\nDisillusioned with SAFe, I found an opportunity to be a product manager for a data analytics team. I took a risk and pursued it, and the manager took a risk on me and hired me. That\u0026rsquo;s where our story begins, and the impetus for this website. I\u0026rsquo;m learning a lot, and I want to document what I\u0026rsquo;m learning.\nPersonal Background I spend the bulk of my days in front of a computer. When I\u0026rsquo;m not here, I like to be outside. I enjoy skiing, ultimate frisbee, Brazilian Jiu Jitsu, kiteboarding, hiking, traveling, and pretty much anything active. I like to do these things with family as much as possible, and I also enjoy amateur videography. That allows me to capture memories in a compelling way that I can enjoy with my wife and kids, and extended family.\n","permalink":"http://localhost:1313/about/","summary":"About Brett Fitzgerald","title":"About Me"},{"content":"Why? Does the internet need another blog? Definitively, no. Do I have original insights that you will benefit from reading? Most likely not. So what\u0026rsquo;s the point of this blog?\nI recently changed jobs, and I\u0026rsquo;m learning a lot. I retain information better when I describe it to someone. I also have a fear that if I constantly regurgitate my ongoing education to my close family, they will eventually want to murder me. That\u0026rsquo;s where this blog comes in. I\u0026rsquo;m going to teach you what I\u0026rsquo;m learning, so I can learn it better.\nInevitably, I\u0026rsquo;ll forget about this blog. Posts will become less frequent, and then stop completely. At some point, I\u0026rsquo;ll just stop writing here completely. After a while, I\u0026rsquo;ll find a new use for my domain name, and this will cease to exist, except in the ever growing dataset of archive.org. So, let\u0026rsquo;s get on with it!\n","permalink":"http://localhost:1313/posts/new-blog/","summary":"Why? Does the internet need another blog? Definitively, no. Do I have original insights that you will benefit from reading? Most likely not. So what\u0026rsquo;s the point of this blog?\nI recently changed jobs, and I\u0026rsquo;m learning a lot. I retain information better when I describe it to someone. I also have a fear that if I constantly regurgitate my ongoing education to my close family, they will eventually want to murder me.","title":"New Blog"}]