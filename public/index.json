[{"content":"A videography hobbyist In my free time, I like shooting videos of my family\u0026rsquo;s adventures and doing some basic editing. I shoot on my cellphone and a GoPro. For the past several years, I have rendered my final projects in 1080p at 24 frames per second. I liked the ability to shoot in 4k and still \u0026ldquo;zoom in\u0026rdquo; digitally to 1080. That also let me shoot slow motion video at 1080, and match my final render resolution. I recently got a newer GoPro, so now I can shoot 4k at 120 fps, which allows me slow down to 20% speed if I render my final project at 24 fps.\nWith this advent of being able to shoot slow motion in 4k resolution, I decided to start rendering my projects in 4k, by default. I\u0026rsquo;m also experimenting with doing very quick edits, just splicing the day\u0026rsquo;s footage together, applying automatic color balancing per-clip, and then rendering at 60fps. This is more of a \u0026ldquo;home movie\u0026rdquo; of a day or event, rather than a curated, edited highlight video. I do all this in Davinci Resolve. Previously, I would be able to render my 1080 videos at 24fps and be happy with the file size and quality of picture. Now that I am rendering at four times the resolution and two and a half times the framerate, my output filesize has ballooned, and I need to pay better attention to my compression.\nI want to find a good balance between output filesize and quality for my home movies.\nComparison of projects In the past, I would shoot for around 100 megabytes per minute of rendered video. So a 5 minute video, rendered at a resolution of 1080, at 24 frames per second would come in around 500 MB. For videos I really spent time on, I would bump up my quality settings and I\u0026rsquo;d be happy with a 3 gig file for a 5 minute video.\nI recently rendered a video using Davinci Resolve\u0026rsquo;s 4k \u0026ldquo;Master\u0026rdquo; preset. So a resolution of 4k, at 60 frames per second, and a duration of 22 minutes came in at a whopping 75 gigabytes (~3.4 GB / min). I used Resolve\u0026rsquo;s \u0026ldquo;YouTube\u0026rdquo; preset, and that reduced the filesize to 1.8 GB.(~80 MB / min). That is a significant difference!\nFor reference, my input files, shot on the GoPro Hero 13 were shot in 4k, mostly at 120 fps. They totaled 18.2GB, so my rendered file was actually four times larger than my source material!\nThe two questions I have are:\nWhat is the difference in output files between these two? Is there a noticeable difference in visible quality? Differences in objective data I wrote a python script that compares various aspects of the videos. I also ran them through the various presets in Handbrake to see how they compare. The video is 21:49 long. These are the results of that comparison, in order of increasing filesize.\nFilename Bitrate (kbps) Resolution Framerate (FPS) Video Codec Filesize Source Video (Sample).MP4 120000 3840x2160 119.88 hevc 67 MB Resolve - YouTube - h264.mp4 11618 3840x2160 60 h264 1.8 GB Resolve - YouTube - h265.mp4 10566 3840x2160 60 hevc 1.7 GB Resolve - Master - h264 - HandBrake - Fast.mp4 37948 3840x2160 60 hevc 6 GB Resolve - Master - h264 - HandBrake - VeryFast.mp4 41025 3840x2160 60 hevc 6.5 GB Resolve - Master - h264 - HandBrake - HQ.mp4 57001 3840x2160 60 hevc 9 GB Resolve - Master - h264 - HandBrake - Super HQ.mp4 78210 3840x2160 60 hevc 12.5 GB Resolve - Master - h265.mp4 473927 3840x2160 60 h264 75.7 GB Resolve - Master - h264.mp4 474208 3840x2160 60 h264 75.8 GB Obviously, Handbrake is going a good job at compressing the video, and lowering the bitrate, thus reducing filesize. But how do the videos actually look?\nSubjective comparison Here are images captured from each of the above videos.\nDavinci Resolve, YouTube preset, h264 Davinci Resolve, YouTube Preset, h265 Handbrake, Fast Handbrake, Very Fast Handbrake, High Quality Handbrake, Super High Quality Davinci Resolve, Master - h264 Davinci Resolve, Master - h265 In these very specific examples, you can immediately see that the DaVinci Resolve YouTube presets are not good. The h264 version shows artifiacts on the right side of the frame and all detail in the snow on the ground is completely lost. Interestingly, the h265 codec doesn\u0026rsquo;t lose as much detail, and is slightly smaller. In the case were you need a small file, it seems like the h265 does a better job at these lower bitrates.\nWhen we jump up into the Handbrake re-encodes, things get noticeably better. Honestly, from the still frames it\u0026rsquo;s very hard (for me) to tell the difference between these images, all the way through to the masters. Even when I watch the playback of the videos themselves, I\u0026rsquo;m hard pressed to see any difference. It could be that the source clips themselves only have a 120Mbps bitrate, and we\u0026rsquo;re re-encoding at a higher bitrate for the masters (474Mbps).\nConclusions To really judge this fairly, I probably should have rendered everything out from DaVinci Resolve and manually adjusted the bitrate. Based on these tests, though, I\u0026rsquo;m not seeing a noticeable loss in quality between the 474Mbps best quality from Resolve and a re-encoded to 78Mbps in Handbrake. For the time being, I\u0026rsquo;m planning to render out from Resolve and limiting my bitrate to 80Mbps. That\u0026rsquo;s only 590 MB or so per minute of video, which isn\u0026rsquo;t bad for what I\u0026rsquo;m doing with them.\n","permalink":"http://localhost:1313/posts/video-compression-analysis/","summary":"A videography hobbyist In my free time, I like shooting videos of my family\u0026rsquo;s adventures and doing some basic editing. I shoot on my cellphone and a GoPro. For the past several years, I have rendered my final projects in 1080p at 24 frames per second. I liked the ability to shoot in 4k and still \u0026ldquo;zoom in\u0026rdquo; digitally to 1080. That also let me shoot slow motion video at 1080, and match my final render resolution.","title":"Video Compression Analysis"},{"content":"Making Music Videos I am very formulaic in how I make videos for my personal library.\nShoot some footage on my cell phone and/or a GoPro. Find a song that captures the feeling of the event. Import all the clips from the event to DaVinci Resolve, and the song selected above. Add the song to the timeline. I then go through the clips, usually chronologically, and see which ones are \u0026ldquo;cool enough\u0026rdquo; to make it the cut. If I end up with more clips than I have time for, ruthlessly cut clips until I have the right amount for the song. Manually drag clip start and end points to align with the measures of the song. Apply Autolevels to each clip, based on the midpoint of the clip (I\u0026rsquo;m red/green color blind and haven\u0026rsquo;t invest any time into figuring out how to color grade). Export and Upload. It\u0026rsquo;s time consuming, but I usually enjoy it. One part that is always frustrating, though, is aligning the cuts in the clips to the measures in the song. I play the timeline and tap the beat on my desk, usually counting out the beats. If I\u0026rsquo;m lucky, there is a visible spike in the waveform on the beats, so I can drag my video clip boundaries to align with them. Sometimes I just have to guess where they are. The whole point is that a generic video like this will draw someone in mre effectively if the video is synchronized with the audio. It\u0026rsquo;s hard to describe, but without the synchronization, the video just sort of plays and the music creates a general feeling. When the timings line up, it feels like the music is pushing the visuals forward, creating more of an emotional resonance with the viewer. That\u0026rsquo;s my experience, at least.\nI wanted to figure out how I could make it easier to find those beats in the audio, and the measure boundaries, so I could speed my workflow.\nMaking a click-track My first attempt was to use python to analyize the audio file of the song I was using. I figured that there is likely some library out there that would determine the beats and hopefully the measures. I could then use that information to generate a new audio file. A single \u0026ldquo;click\u0026rdquo; sound on the beats, and a higher pitched click on the first beat of each measure. With that audio file, I could add it to the timeline in Resolve, mute it, and have a very clear visual to align my video clips to.\nWith some help from Claude.ai and ChatGPT, I played with the librosa libary. It was suprisingly accurate at finding the beats and generating a click track. The biggest hurdle I ran into happened during musical transitions. It seems that librosa analyizes the music for acoustic \u0026ldquo;density\u0026rdquo;. Music is generally more \u0026ldquo;dense\u0026rdquo; on the beats, which is where louder drums are hit, guitar chords are strummed, keys are struck, and generally most or all of the instruments \u0026ldquo;happen\u0026rdquo; at the same time.\nFor example, a guitar chord would be played on a beat, and then it is left to resonate until the next beat, or possibly a divison of the next beat. So with most of the instruments striking on the beats, those moments are more dense. Any given song will usually have a relatively consistent feel, and it seems like librosa does a good job of determining that feel, finding those dense moments and aligning them to the beats. Where it struggled for me is when the general density of the music changed.\nI was testing with The Allman Brother\u0026rsquo;s song Ramblin\u0026rsquo; Man. This song starts with a clearly defined introduction which is mainly electric guitar for a couple of measures, and then the full band comes in. The problem is that librosa seems to normalize to the relatively low density of the electric guitar-only intro, and correctly identifies the beats. But once the full band comes in, the density goes up significantly, and librosa identifies extra beats for a short period, until it renormalizes to the higher density of the song. That throws off the counts of our beats, and then our measures are no longer in alignment.\nI played with several parameters of librosa, but never got to satisfactory results.\nMaking project markers DaVinci Resolve (and most other populary video editing tools) support a feature called Markers. I haven\u0026rsquo;t used them before, so this is my first foray into them. The idea is that you can set specific markers on your timeline and then snap clips to them.\n","permalink":"http://localhost:1313/posts/automatic-music-markers-in-davinci-resolve/","summary":"Making Music Videos I am very formulaic in how I make videos for my personal library.\nShoot some footage on my cell phone and/or a GoPro. Find a song that captures the feeling of the event. Import all the clips from the event to DaVinci Resolve, and the song selected above. Add the song to the timeline. I then go through the clips, usually chronologically, and see which ones are \u0026ldquo;cool enough\u0026rdquo; to make it the cut.","title":"Automatic Music Markers in Davinci Resolve"},{"content":"Building a large language model from scratch I\u0026rsquo;m a machine learning / A.I. hobbyist. The technologies fascinate me, and I can\u0026rsquo;t seem to learn enough about them. Sebastian Raschka\u0026rsquo;s book, Build a Large Language Model (From Scratch) caught my eye. I don\u0026rsquo;t recall how I stumbled on it, but I found it when it was still in early access from Manning Publications. I purchased it, and started working through it as the final chapters were being written and released. I just completed the book and all the included work and loved every minute of it.\nMy approach A while ago, I read some advice about learning programming from digital books and tutorials. The advice was to never copy and paste code from samples but to hand-type all the code. I took that approach with this book. I typed every single line of code (except for a couple of blocks which were highly repetitive and long). You can see all my work here: https://github.com/controversy187/build-a-large-language-model\nI did my best to work in section chunks. I didn\u0026rsquo;t want to start a section unless I had the time dedicated to completing it. Some sections are pretty short, others are fairly involved and time-consuming.\nI built this in Jupyter Notebooks on my laptop, which is pretty underpowered for this type of work. The premise of the book was that you can build an LLM on consumer hardware, and it can perform decently well. As I\u0026rsquo;m writing this, I\u0026rsquo;m currently fine-tuning my model locally. My model is about 50 steps into a 230-step tuning, and I just crossed the 20-minute execution time mark. The earlier code samples ran quicker, but the last few sections used larger models, which slowed things down considerably.\nI didn\u0026rsquo;t do most of the supplemental exercises. I tend to have an \u0026ldquo;I want to do ALL THE THINGS!\u0026rdquo; personality. The drawback is that if I take the time to do all the things, I eventually get long-term distracted and never actually finish what I started. So I sort of rushed through this book. I even took several weeks off around Christmas and New Year\u0026rsquo;s. But I got back into it and powered through the last few chapters.\nSo, more or less, I read through the chapters and wrote all the mandatory coding assignments.\nLearnings What can I tell you about large language models? A lot more than I could before I started this book, but certainly not all the things the author attempted to teach me. I\u0026rsquo;ll summarize my understanding, but I could be wrong about some of these things, and I most certainly forgot or misunderstood others.\nTokenization \u0026amp; Vocabulary A large language model starts its life by building a vocabulary of text. A massive amount of text is distilled down into a list of unique words. Each word is then translated into an integer because computers like numbers more than they like words. This process is referred to as \u0026ldquo;tokenization\u0026rdquo;, where the word is replaced with a numerical token. So now we have a list of unique tokens, which is the vocabulary of the large language model.\n# Build a more advanced tokenizer text = \u0026#34;Hello, world. Is this-- a test?\u0026#34; result = re.split(r\u0026#39;([,.:;?_!\u0026#34;()\\\u0026#39;]|--|\\s)\u0026#39;, text) result = [item.strip() for item in result if item.strip()] print(result) # Outputs \u0026#34;[\u0026#39;Hello\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;world\u0026#39;, \u0026#39;.\u0026#39;, \u0026#39;Is\u0026#39;, \u0026#39;this\u0026#39;, \u0026#39;--\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;test\u0026#39;, \u0026#39;?\u0026#39;]\u0026#34; all_words = sorted(set(result)) vocab_size = len(all_words) print(vocab_size) # Outputs 10 # Display the first 51 tokens in our vocabulary. vocab = {token:integer for integer,token in enumerate(all_words)} for i, item in enumerate(vocab.items()): print(item) # Outputs: (\u0026#39;,\u0026#39;, 0) (\u0026#39;--\u0026#39;, 1) (\u0026#39;.\u0026#39;, 2) (\u0026#39;?\u0026#39;, 3) (\u0026#39;Hello\u0026#39;, 4) (\u0026#39;Is\u0026#39;, 5) (\u0026#39;a\u0026#39;, 6) (\u0026#39;test\u0026#39;, 7) (\u0026#39;this\u0026#39;, 8) (\u0026#39;world\u0026#39;, 9) # In this example, the id 9 represents the word \u0026#34;world\u0026#34;. 5 represents \u0026#34;Is\u0026#34;. etc. This is where my understanding gets fuzzy. We didn\u0026rsquo;t get very far before that happened, \u0026rsquo;eh? Now, we take that massive amount of text we were using earlier to create the vocabulary (or a subset, or totally different text), and we tokenize the entire text. We do this by using the vocabulary we built previously and substituting the words in the training text for their equivalent token value. This is now our training text.\nModel Training \u0026amp; Relationships With that complete, we can \u0026ldquo;train\u0026rdquo; the model. This process involves taking each token in the vocabulary and building a relationship to each other token in the vocabulary, based on those tokens\u0026rsquo; relative positions to each other in the training text. So if the word \u0026ldquo;cat\u0026rdquo; is followed by the word \u0026ldquo;jump\u0026rdquo;, the model records that relationship. But it also records the relationship of the word \u0026ldquo;cat\u0026rdquo; to other words in the text. So \u0026ldquo;jump\u0026rdquo; follows \u0026ldquo;cat\u0026rdquo;, but maybe it does so more frequently when they are close to the word \u0026ldquo;mouse\u0026rdquo;. And maybe less frequently when they are close to the word \u0026ldquo;nap\u0026rdquo;. Recording ALL these relationships would require a massive dataset, so the relationships are mathematically reduced and approximated. There are definitely more technical terms to use, and the book went into them. I definitely forget them, though.\nText Generation Process Now, if you provide a starter text to the model, it will try to complete the text for you. Continuing our example, if I gave the model the text \u0026ldquo;My cat saw a mouse and it\u0026rdquo;, based on the word cat being close to the word mouse, it might predict the word \u0026ldquo;jumped\u0026rdquo; to come next. So it appends the word \u0026ldquo;jumped\u0026rdquo; to the text I submitted, and then it takes that whole new sentence and feeds it back into itself. So now the input text is \u0026ldquo;My cat saw a mouse and it jumped\u0026rdquo;. The next output word could be \u0026ldquo;on\u0026rdquo;, so it appends that word and feeds this concatenated output back into its input.\nEvery time it does a loop like this, it tokenizes the entire input (or up to a limit, known as a context limit or context window) and then calculates the most likely next token, then converts it all back to text for us to read.\nModel Weights \u0026amp; Distribution Saving all those relationships between the tokens are known as the \u0026ldquo;weights\u0026rdquo; of the model. Those can be distributed, so if you train a model on a given training text, you can give that to your friends and they can use those model weights to predict text similar to that training text.\nFine Tuning Fine-tuning is the process of training a model for specific\u0026hellip; things. My mind is getting fuzzier here, so I\u0026rsquo;m not going to go into this deeper. Suffice it to say, that you start with a base language model and continue to train it using specific input and output pairs. In the book, we built a spam classifier that determined if a given message was spam or not, as well as a model that will follow instructions. That\u0026rsquo;s actually the one that\u0026rsquo;s being trained right now as I write this post, so I\u0026rsquo;m not sure how it will turn out. Based on the fact that it\u0026rsquo;s published in a book, I think it will come out just fine.\nSo while I\u0026rsquo;m not completely done with the book, I\u0026rsquo;m very nearly there. I did learn a lot of great concepts, although obviously some of them weren\u0026rsquo;t retained. It would probably behoove me to go back through the book again and quickly breeze through it, in order to refresh my memory and cement my learnings.\nMeta learnings Other than the technical aspects of Large Language Models, what else did I learn through this experience?\nThrough my experiment with typing all the code samples by hand, I can say that my time would have been better spent with a different approach. If I do this again, I\u0026rsquo;ll probably not type all the code snippets, but rather \u0026ldquo;type\u0026rdquo; them in my mind, and really understand what each line does. The times I learned the most were actually when I made a typo and had to go back through my code to debug it. That forced me to understand what was happening so I could figure out what went wrong.\nI learn better with paper, rather than a digital book. I don\u0026rsquo;t know why. I had both available to me, and I read the first couple of chapters in the paper book. That information stuck better. Maybe because it was earlier in the book and simpler to understand, or maybe the format played into it. But I enjoyed it better, regardless.\nI didn\u0026rsquo;t have to \u0026ldquo;figure out\u0026rdquo; anything, and I think that hampered my learning. There are supplemental exercises in the book, where the author gives you a problem and you have to figure out how to solve it. The answers are given in his GitHub repository. That would have slowed me down a lot, but I\u0026rsquo;m very confident that I would have learned the material better.\nWhat\u0026rsquo;s next? I\u0026rsquo;m torn right now. I want to understand this material better, but I wonder if getting into lower-level, specific material might help me understand AI and machine learning better. What will likely happen is that I\u0026rsquo;ll copy and paste this content into Claude.ai and suggest a path forward for me.\n","permalink":"http://localhost:1313/posts/build-a-large-language-model/","summary":"Building a large language model from scratch I\u0026rsquo;m a machine learning / A.I. hobbyist. The technologies fascinate me, and I can\u0026rsquo;t seem to learn enough about them. Sebastian Raschka\u0026rsquo;s book, Build a Large Language Model (From Scratch) caught my eye. I don\u0026rsquo;t recall how I stumbled on it, but I found it when it was still in early access from Manning Publications. I purchased it, and started working through it as the final chapters were being written and released.","title":"Build a Large Language Model From Scratch"},{"content":"I\u0026rsquo;m a technologist who\u0026rsquo;s followed my curiosity through various roles in software development, agile leadership, and now data analytics. This blog documents my ongoing journey of learning and exploration in technology.\nProfessional Background I got my start in computers by hacking around on a Commodore 64. I distinctly remember loading an ASCII bowling game, and finding out that I could see the source code before running the program. Not knowing what source code was at the time, I simply saw the word \u0026ldquo;BOWLING\u0026rdquo;, as it appeared on the title screen. Being in elementary school at the time, I changed it to \u0026ldquo;POOPING\u0026rdquo;, ran the program, saw my changes, and my life was set on a very specific course.\nFrom these humble beginnings, I went on to study computer networking through college. Upon graduating, I immediately began a career in web development, because CISCO certifications are hard. Early career HTML, CSS, JS, Classic ASP and PHP got me started in the world of software development. As technologies evolved, I barely did, continuing my skillset and going moderately deep in the PHP and WordPress world. I dabbled in AWS architecture in the early AWS days, and eventually found more joy at the intersection of people and technology than I did staying heads down at the keyboard.\nThose interests led me into the world of Agile, and I became a Scrum Master. I loved working with a small focused team, and we did a lot of things that weren\u0026rsquo;t really Scrum, but worked well for us. I ended up drinking the Scaled Agile Framework Cool-Aid, and even did a little SAFe consulting. This led me to Portfolio Manager role.\nTrying to scale Agile was a frustrating experience. I wasn\u0026rsquo;t close to the work, and I didn\u0026rsquo;t feel like I was providing value. In my free time, I was learning about machine learning, artificial intelligence, blockchain technologies, and other technological interests. My passions didn\u0026rsquo;t lie with the Scaled Agile Framework.\nDisillusioned with SAFe, I found an opportunity to be a product manager for a data analytics team. I took a risk and pursued it, and the manager took a risk on me and hired me. That\u0026rsquo;s where our story begins, and the impetus for this website. I\u0026rsquo;m learning a lot, and I want to document what I\u0026rsquo;m learning.\nPersonal Background I spend the bulk of my days in front of a computer. When I\u0026rsquo;m not here, I like to be outside. I enjoy skiing, ultimate frisbee, Brazilian Jiu Jitsu, kiteboarding, hiking, traveling, and pretty much anything active. I like to do these things with family as much as possible, and I also enjoy amateur videography. That allows me to capture memories in a compelling way that I can enjoy with my wife and kids, and extended family.\n","permalink":"http://localhost:1313/about/","summary":"About Brett Fitzgerald","title":"About Me"},{"content":"Why? Does the internet need another blog? Definitively, no. Do I have original insights that you will benefit from reading? Most likely not. So what\u0026rsquo;s the point of this blog?\nI recently changed jobs, and I\u0026rsquo;m learning a lot. I retain information better when I describe it to someone. I also have a fear that if I constantly regurgitate my ongoing education to my close family, they will eventually want to murder me. That\u0026rsquo;s where this blog comes in. I\u0026rsquo;m going to teach you what I\u0026rsquo;m learning, so I can learn it better.\nInevitably, I\u0026rsquo;ll forget about this blog. Posts will become less frequent, and then stop completely. At some point, I\u0026rsquo;ll just stop writing here completely. After a while, I\u0026rsquo;ll find a new use for my domain name, and this will cease to exist, except in the ever growing dataset of archive.org. So, let\u0026rsquo;s get on with it!\n","permalink":"http://localhost:1313/posts/new-blog/","summary":"Why? Does the internet need another blog? Definitively, no. Do I have original insights that you will benefit from reading? Most likely not. So what\u0026rsquo;s the point of this blog?\nI recently changed jobs, and I\u0026rsquo;m learning a lot. I retain information better when I describe it to someone. I also have a fear that if I constantly regurgitate my ongoing education to my close family, they will eventually want to murder me.","title":"New Blog"}]